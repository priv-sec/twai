{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dpsgd_twai.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python377jvsc74a57bd000fe828615db63367d4538dea818edeccf45ed831be4f445733eae33643b3f7b",
      "display_name": "Python 3.7.7 64-bit"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.7"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyfuZbODbcjn"
      },
      "source": [
        "#Exercise\n",
        "\n",
        "a) In this scenaro, we want to use the norm of the mean of the gradients as clipping bound. Please complete the implementation of the clipping function in the code below. \n",
        "\n",
        "*Note: By using the norm of the mean of the gradients as clipping bound you should reach an accuracy value above 90% after 10 epochs.*\n",
        "\n",
        "b) Now, it is your turn to think of a way to ensure privacy during the training of our neural network. The norm of the mean of the gradients introduces some leakage which we want to avoid. How can we get rid of this kind of leakage? Please implement your solution in the code below.\n",
        "\n",
        "Please make sure to select **GPU** under **Runtime -> Change Runtime Type -> Hardware Accelerator**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoVSok-KXPL3"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOJCKTVAVFq1"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch.nn.utils import clip_grad_norm_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oo_uYySLXUnj"
      },
      "source": [
        "### Define our model/CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQ-nBMc7eRpZ"
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        num_hidden = 512\n",
        "        num_classes = 10\n",
        "        self.fc1 = nn.Linear(28*28, num_hidden)\n",
        "        self.fc2 = nn.Linear(num_hidden, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, self.num_flat_features(x))\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def num_flat_features(self, x):\n",
        "      size = x.size()[1:]\n",
        "      num_features = 1\n",
        "      for s in size:\n",
        "        num_features *= s\n",
        "      return num_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYF1ukzWXak5"
      },
      "source": [
        "### Function to download our datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDQi-MTFeZZF"
      },
      "source": [
        "def get_mnist_dataset(root_dir='../data'):\n",
        "    transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                    transforms.Normalize((0.1307,), (0.3081,))])\n",
        "    train_dataset = datasets.MNIST(root_dir, train=True, download=True, transform=transform)\n",
        "    test_dataset = datasets.MNIST(root_dir, train=False, download=True, transform=transform)\n",
        "\n",
        "    return train_dataset, test_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FH2KftdsxbjV"
      },
      "source": [
        "\n",
        "##################\n",
        "#   Exercise a)  #\n",
        "##################\n",
        "\n",
        "def clip(gradient, c_bound):\n",
        "   new_gradient = # TODO #\n",
        "   return new_gradient\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghKo5IQqX0TZ"
      },
      "source": [
        "### Define our training function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqSjocRUdXQW"
      },
      "source": [
        "def train(model, device, train_loader, optimizer, epoch, noise_multiplier):\n",
        "    model.train()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    losses = []\n",
        "    top1_acc = []\n",
        "    \n",
        "    # Loop over all batches in our training set\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        \n",
        "        # List to store calculated gradients\n",
        "        grad_list = [ [] for _ in model.parameters() ]\n",
        "        \n",
        "        # Loop over all data points in the current batch\n",
        "        for i in range(len(data)):\n",
        "            # Add empty dimension to fit expected shape\n",
        "            x = data[i].unsqueeze_(0)\n",
        "            y = target[i].unsqueeze_(0)\n",
        "\n",
        "            # Reset model's current gradients\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Let model predict label of current data point\n",
        "            y_hat = model(x)\n",
        "            \n",
        "            # Compare prediction to true label, calculate loss\n",
        "            loss = criterion(y_hat, y)\n",
        "            pred = torch.argmax(y_hat.detach())\n",
        "            label = y.detach()\n",
        "\n",
        "            # Calculate model's accuracy for current data point\n",
        "            acc1 = torch.eq(pred, label).float()\n",
        "\n",
        "            # Save performance values\n",
        "            losses.append(loss.item())\n",
        "            top1_acc.append(acc1)\n",
        "\n",
        "            # Perform back propagation / calculate gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # Save gradients for current data point in grad_list\n",
        "            for j, p in enumerate(model.parameters()):\n",
        "                grad_list[j].append(p.grad.detach().clone())\n",
        "\n",
        "        # Reset model's current gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        for j, param in enumerate(model.parameters()):\n",
        "            # Clip length of gradients\n",
        "            \n",
        "            ######################################\n",
        "            # Use norm of mean as clipping bound #\n",
        "            ######################################\n",
        "            grad_stack = torch.stack(grad_list[j])\n",
        "            mean_grad = torch.mean(grad_stack, dim=0)\n",
        "            max_grad_norm = torch.norm(mean_grad)\n",
        "            grad_list[j] = [clip(g, max_grad_norm) for g in grad_list[j]]\n",
        "\n",
        "            # Compute mean of gradients\n",
        "            grad_stack = torch.stack(grad_list[j])\n",
        "            mean_grad = torch.mean(grad_stack, dim=0)\n",
        "\n",
        "            # Create noise\n",
        "            zeros = torch.zeros(*mean_grad.size()).to(device)\n",
        "            normal = torch.distributions.Normal(zeros, torch.tensor(noise_multiplier * max_grad_norm).to(device))\n",
        "\n",
        "            # Sample from noise distribution and add to mean gradient\n",
        "            param.grad = mean_grad.add(torch.div(normal.sample(), train_loader.batch_size))\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "    # Calculate average loss and accuracy of current epoch\n",
        "    avg_loss = torch.mean(torch.tensor(losses)).detach().cpu().numpy()\n",
        "    avg_acc = torch.mean(torch.tensor(top1_acc)).detach().cpu().numpy()\n",
        "    \n",
        "    return avg_loss, avg_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXQya988X7Xm"
      },
      "source": [
        "### Define our test function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tQTOPZNgWhj"
      },
      "source": [
        "def accuracy(preds, labels):\n",
        "  return (preds == labels).mean()\n",
        "\n",
        "\n",
        "def test(model, device, test_loader, epoch):\n",
        "  model.eval()\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  losses = []\n",
        "  top1_acc = []\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "      data, target = data.to(device), target.to(device)\n",
        "      output = model(data)\n",
        "      test_loss = criterion(output, target)\n",
        "      preds = np.argmax(output.detach().cpu().numpy(), axis=1)\n",
        "      labels = target.detach().cpu().numpy()\n",
        "      acc1 = accuracy(preds, labels)\n",
        "      \n",
        "      losses.append(test_loss.item())\n",
        "      top1_acc.append(acc1)\n",
        "  \n",
        "  test_loss = np.mean(losses)\n",
        "  top1_avg = np.mean(top1_acc)\n",
        "    \n",
        "  return test_loss, top1_avg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAqrQLFGYHWM"
      },
      "source": [
        "### Set training parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADgdN_FRgxmI"
      },
      "source": [
        "LR = 5e-3\n",
        "TRAIN_BATCHSIZE = 32\n",
        "TEST_BATCHSIZE = 1000\n",
        "\n",
        "EPOCHS = 10\n",
        "\n",
        "SIGMA = 1.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ow0bVb9RYOH5"
      },
      "source": [
        "### Download dataset, initialize model, create optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JozWfUkKelCi"
      },
      "source": [
        "cuda_available = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if cuda_available else \"cpu\")\n",
        "\n",
        "train_kwargs = {\n",
        "    'batch_size': TRAIN_BATCHSIZE\n",
        "    }\n",
        "\n",
        "test_kwargs = {\n",
        "    'batch_size': TEST_BATCHSIZE\n",
        "    }\n",
        "\n",
        "if cuda_available:\n",
        "    cuda_kwargs = {\n",
        "        'num_workers': 2,\n",
        "        'pin_memory': True,\n",
        "        'shuffle': True\n",
        "        }\n",
        "    train_kwargs.update(cuda_kwargs)\n",
        "    test_kwargs.update(cuda_kwargs)\n",
        "\n",
        "train_dataset, test_dataset = get_mnist_dataset()\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, **train_kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, **test_kwargs)\n",
        "\n",
        "model = Model().to(device)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=LR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwRkn2AMYkQX"
      },
      "source": [
        "### Perform training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0rzkopIf_um"
      },
      "source": [
        "train_losses = []\n",
        "test_losses = []\n",
        "train_accs = []\n",
        "test_accs = []\n",
        "\n",
        "for epoch in tqdm(range(1, EPOCHS+1), desc=\"Epoch\", unit=\"epoch\"):\n",
        "  # Perform training step\n",
        "  train_loss, train_acc = train( model, device, train_loader, optimizer, epoch, SIGMA)\n",
        "  \n",
        "  # Perform test step\n",
        "  test_loss, test_acc = test(model, device, test_loader, epoch)\n",
        "\n",
        "  # Print results\n",
        "  print(f\"Epoch: {epoch}, Avg Train Loss: {train_loss:.4f}, Train Acc: {(100. * train_acc):.2f}%\")\n",
        "  print(f\"Epoch: {epoch}, Avg Test Loss: {test_loss:.4f}, Test Acc: {(100. * test_acc):.2f}%\\n\")\n",
        "\n",
        "  # Save performance progress\n",
        "  train_losses.append(train_loss)\n",
        "  test_losses.append(test_loss)\n",
        "  train_accs.append(train_acc)\n",
        "  test_accs.append(test_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygrL8HIjzw_K"
      },
      "source": [
        "### Plot results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOdREAjhhk5E"
      },
      "source": [
        "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2)\n",
        "fig.set_size_inches(w=15,h=5)\n",
        "ax1.plot(train_losses, label=\"Train Loss\")\n",
        "ax1.plot(test_losses, label=\"Test Loss\")\n",
        "ax1.set_xlabel(\"Epoch\")\n",
        "ax1.set_ylabel(\"Loss\")\n",
        "ax1.legend()\n",
        "ax2.plot(train_accs, label=\"Train Accuracy\")\n",
        "ax2.plot(test_accs, label=\"Test Accuracy\")\n",
        "ax2.set_xlabel(\"Epoch\")\n",
        "ax2.set_ylabel(\"Accuracy\")\n",
        "ax2.set_ylim(0, 1)\n",
        "ax2.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wy3MJdsW2jbj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}